# -*- coding: utf-8 -*-
"""StylaQuiz.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12MP6RY1CIdN1FhP5eVZE-nH18bQeiyQ_
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture --no-stderr
# %pip install -q "colpali_engine>=0.3.1" "datasets" "huggingface_hub[hf_transfer]" "transformers>=4.45.0" "qdrant-client"
# %pip install fsspec==2024.10.0
# %pip install protobuf==3.20.3
# %pip install google-generativeai
# %pip install stamina
# %pip install pycryptodome

import os

# FORCE ALL HUGGINGFACE OPERATIONS TO USE YOUR DIRECTORY
local_dir = r"E:\StylaQuiz\colqwen2_model"

# Set environment variables BEFORE any HuggingFace imports
os.environ["HF_HOME"] = local_dir
os.environ["TRANSFORMERS_CACHE"] = local_dir
os.environ["HF_HUB_CACHE"] = local_dir
os.environ["HF_DATASETS_CACHE"] = local_dir
os.environ["TORCH_HOME"] = local_dir

print(f"üè† Forced HuggingFace cache to: {local_dir}")
print(f"üè† HF_HOME: {os.environ.get('HF_HOME')}")
print(f"üè† TRANSFORMERS_CACHE: {os.environ.get('TRANSFORMERS_CACHE')}")

from colpali_engine.models import ColPali, ColPaliProcessor, ColQwen2, ColQwen2Processor
from datasets import load_dataset
# from google.colab import userdata
from qdrant_client import QdrantClient
from qdrant_client.http import models
import torch,uuid
from tqdm import tqdm
import base64
from io import BytesIO
import tempfile
import shutil

import json
import numpy as np
from PIL import Image
import requests

from qdrant_client import QdrantClient

qdrant_client = QdrantClient(
    url="https://f176825b-e7eb-469c-a351-a4ee70d8a54c.us-east-1-1.aws.cloud.qdrant.io:6333",
    api_key="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.fD_dT8Nyi0z-hibrnBrIx-YTNGenRRe3xgFiatufUsw",
)

from huggingface_hub import snapshot_download
import json

def check_available_space(path):
    """Check available space in GB for a given path (cross-platform)"""
    try:
        # Check the directory of the path, not the path itself
        check_path = os.path.dirname(path) if not os.path.exists(path) else path
        total, used, free = shutil.disk_usage(check_path)
        available_space_gb = free / (1024**3)
        return available_space_gb
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not check disk space: {e}")
        return float('inf')  # Assume infinite space if we can't check

def setup_temp_directory():
    """Set up temporary directory in your E: drive"""
    temp_dir = os.path.join(os.path.dirname(local_dir), "temp_download")
    os.makedirs(temp_dir, exist_ok=True)
    return temp_dir

def is_model_complete(local_dir):
    """
    Check if model download is complete - more flexible for different model types
    """
    if not os.path.exists(local_dir):
        print("üìÅ Model directory doesn't exist")
        return False
    
    print(f"üîç Checking model completeness in: {local_dir}")
    
    # List all files for debugging
    all_files = os.listdir(local_dir)
    print(f"üìÅ Found {len(all_files)} files:")
    for f in sorted(all_files)[:10]:
        print(f"   üìÑ {f}")
    if len(all_files) > 10:
        print(f"   ... and {len(all_files) - 10} more files")
    
    # Check for essential files (be flexible)
    essential_files = ["tokenizer.json"]
    missing_essential = []
    
    for file in essential_files:
        file_path = os.path.join(local_dir, file)
        if not os.path.exists(file_path):
            missing_essential.append(file)
        elif os.path.getsize(file_path) == 0:
            missing_essential.append(f"{file} (empty)")
    
    if missing_essential:
        print(f"‚ùå Missing essential files: {missing_essential}")
        return False
    
    # Check for model weight files (flexible patterns)
    model_files = [f for f in all_files if f.endswith('.safetensors')]
    if len(model_files) == 0:
        print("‚ùå No model weight files found")
        return False
    
    # Check total size
    total_model_size = 0
    for model_file in model_files:
        file_path = os.path.join(local_dir, model_file)
        file_size = os.path.getsize(file_path)
        total_model_size += file_size
        
    print(f"üì¶ Found {len(model_files)} model files, total size: {total_model_size / (1024**2):.1f} MB")
    
    # Expect at least 50MB total (realistic for adapter models)
    if total_model_size < 50 * 1024 * 1024:
        print(f"‚ùå Total model size too small: {total_model_size / (1024**2):.1f} MB")
        return False
    
    print("‚úÖ Model appears to be complete")
    return True

# Check available space on E: drive
e_drive_space = check_available_space(local_dir)
print(f"üíæ Available space on E: drive: {e_drive_space:.2f} GB")

if e_drive_space < 12:  # Need at least 12GB for safe download
    print("‚ö†Ô∏è  Warning: Less than 12GB available on E: drive. Model download might fail.")
    print("üîß Consider freeing up space on your E: drive.")

model_name = "vidore/colqwen2-v0.1"

print(f"üîç Checking for existing model at: {local_dir}")

# Check if model already exists and is complete
if is_model_complete(local_dir):
    print(f"‚úÖ Complete model found! Skipping download.")
    should_download = False
else:
    print("üì• Model not found or incomplete. Will download...")
    should_download = True
    
    # Clean up any incomplete download
    if os.path.exists(local_dir):
        print("üßπ Removing incomplete download...")
        shutil.rmtree(local_dir)

# Only download if needed
if should_download:
    # Set up custom temp directory on E: drive
    custom_temp_dir = setup_temp_directory()
    original_tmpdir = tempfile.gettempdir()
    tempfile.tempdir = custom_temp_dir
    
    # Create the directory if it doesn't exist
    os.makedirs(local_dir, exist_ok=True)
    
    print("üì• Starting model download...")
    print("‚è±Ô∏è  This may take several minutes depending on your internet connection...")
    print("üìä Expected model size: ~9GB total")
    print(f"üåê Repository: {model_name}")
    print(f"üíæ Downloading to: {local_dir}")
    print(f"üìÅ Temp directory: {custom_temp_dir}")
    
    # Quick check if repository exists and is accessible
    try:
        from huggingface_hub import repo_info
        repo_data = repo_info(model_name)
        print(f"‚úÖ Repository verified: {repo_data.id}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not verify repository: {e}")
        print("üîÑ Continuing with download anyway...")
    
    try:
        # Download to the specified local directory with ALL cache parameters
        print("üîÑ Starting download with forced local directory...")
        snapshot_download(
            repo_id=model_name,
            local_dir=local_dir,
            local_dir_use_symlinks=False,
            resume_download=True,
            cache_dir=local_dir,  # Force cache to your directory
            local_files_only=False,  # Allow download
            force_download=False  # Don't re-download if exists
        )
        
        print("‚úÖ Download completed!")
        
        # Verify download completed
        if not is_model_complete(local_dir):
            print("‚ö†Ô∏è  Download verification shows some issues, but continuing...")
        
        # 2Ô∏è‚É£ Locate and patch preprocessor_config.json (or preprocessor.json)
        patched = False
        for file_name in ["preprocessor_config.json", "preprocessor.json"]:
            config_path = os.path.join(local_dir, file_name)
            if os.path.exists(config_path):
                try:
                    with open(config_path, "r") as f:
                        cfg = json.load(f)

                    # Patch the size keys
                    if "size" in cfg:
                        original_cfg = dict(cfg["size"])
                        for bad_key in ["max_pixels", "min_pixels"]:
                            cfg["size"].pop(bad_key, None)
                        
                        if cfg["size"] != original_cfg:
                            # Save patched config
                            with open(config_path, "w") as f:
                                json.dump(cfg, f, indent=2)
                            print(f"‚úÖ Patched {file_name}")
                        else:
                            print(f"‚ÑπÔ∏è  {file_name} didn't need patching")
                        patched = True
                        break
                except Exception as e:
                    print(f"‚ö†Ô∏è  Error patching {file_name}: {e}")
        
        if not patched:
            print("‚ÑπÔ∏è  No preprocessor config found to patch (this might be normal)")
        
        print(f"‚úÖ Model successfully downloaded and saved to {local_dir}")
        
    except Exception as e:
        print(f"‚ùå Download failed: {e}")
        print("üîß Try the following:")
        print("   1. Check your internet connection")
        print("   2. Free up more disk space on E: drive")
        print("   3. Run the script again (resume_download=True will continue)")
        print("   4. Delete the colqwen2_model folder and try again")
        if "disk space" in str(e).lower():
            print("   5. The error mentions disk space - check all your drives")
        raise
    finally:
        # Restore original temp directory
        tempfile.tempdir = original_tmpdir
        # Clean up custom temp directory
        if os.path.exists(custom_temp_dir):
            try:
                shutil.rmtree(custom_temp_dir)
                print("üßπ Cleaned up temporary files")
            except:
                pass  # Don't fail if cleanup doesn't work

# 3Ô∏è‚É£ Load model and patched processor
print("üîÑ Loading model and processor from local directory...")

# Check what files we actually have
print("üìã Files downloaded:")
all_files = os.listdir(local_dir)
for f in sorted(all_files):
    file_path = os.path.join(local_dir, f)
    if os.path.isfile(file_path):
        size_mb = os.path.getsize(file_path) / (1024 * 1024)
        print(f"   üìÑ {f} ({size_mb:.1f} MB)")
    else:
        print(f"   üìÅ {f}/")

try:
    # The downloaded model appears to be an adapter, let's try different loading approaches
    print("üîÑ Attempting to load model...")
    
    # First, check if we have a config.json - if not, this is likely an adapter model
    config_path = os.path.join(local_dir, "config.json")
    adapter_config_path = os.path.join(local_dir, "adapter_config.json")
    
    if os.path.exists(adapter_config_path) and not os.path.exists(config_path):
        print("üîç Detected adapter model - trying to load base model first...")
        
        # Reset environment variables to allow network access for base model
        if "HF_HUB_OFFLINE" in os.environ:
            del os.environ["HF_HUB_OFFLINE"]
        
        try:
            # Load base model from HuggingFace, then adapter from local
            print("üì• Loading base ColQwen2 model from HuggingFace...")
            model = ColQwen2.from_pretrained(
                "vidore/colqwen2-v0.1",  # Load base model from HF
                torch_dtype=torch.bfloat16,
                device_map="cuda:0" if torch.cuda.is_available() else "cpu",
                trust_remote_code=True
            ).eval()
            
            print("üì• Loading processor from HuggingFace...")
            processor = ColQwen2Processor.from_pretrained(
                "vidore/colqwen2-v0.1",
                trust_remote_code=True
            )
            
            print("‚úÖ Base model and processor loaded from HuggingFace!")
            
        except Exception as e1:
            print(f"‚ùå Failed to load base model from HuggingFace: {e1}")
            raise e1
            
    else:
        # Try loading directly from local directory
        print("üîÑ Attempting direct local loading...")
        
        # Force offline mode for loading
        os.environ["HF_HUB_OFFLINE"] = "1"
        
        model = ColQwen2.from_pretrained(
            local_dir,
            torch_dtype=torch.bfloat16,
            device_map="cuda:0" if torch.cuda.is_available() else "cpu",
            local_files_only=True,
            cache_dir=local_dir,
            trust_remote_code=True
        ).eval()

        processor = ColQwen2Processor.from_pretrained(
            local_dir,
            local_files_only=True,
            cache_dir=local_dir,
            trust_remote_code=True
        )
        
        # Reset offline mode
        if "HF_HUB_OFFLINE" in os.environ:
            del os.environ["HF_HUB_OFFLINE"]
    
    print("‚úÖ Model and processor loaded successfully!")
    print(f"   üì¶ Model device: {model.device}")
    print(f"   üî¢ Model dtype: {model.dtype}")
    
except Exception as e:
    print(f"‚ùå All loading attempts failed: {e}")
    print("\nüîß Troubleshooting:")
    print("üîç The downloaded files appear to be incomplete or an adapter model.")
    print("üí° Solution options:")
    print("   1. This script will now load the base model from HuggingFace (requires internet)")
    print("   2. Your local files are preserved in E:\\StylaQuiz\\colqwen2_model")
    print("   3. For completely offline use, we need to download the complete base model")
    
    # Final attempt: Load base model from HuggingFace with internet
    try:
        print("\nüåê Final attempt: Loading base model from HuggingFace...")
        
        # Reset all offline modes
        for env_var in ["HF_HUB_OFFLINE", "TRANSFORMERS_OFFLINE"]:
            if env_var in os.environ:
                del os.environ[env_var]
        
        model = ColQwen2.from_pretrained(
            "vidore/colqwen2-v0.1",
            torch_dtype=torch.bfloat16,
            device_map="cuda:0" if torch.cuda.is_available() else "cpu",
            trust_remote_code=True,
            cache_dir=local_dir  # Still cache to your directory
        ).eval()

        processor = ColQwen2Processor.from_pretrained(
            "vidore/colqwen2-v0.1",
            trust_remote_code=True,
            cache_dir=local_dir  # Still cache to your directory
        )
        
        print("‚úÖ Model and processor loaded from HuggingFace with local caching!")
        print(f"   üì¶ Model device: {model.device}")
        print(f"   üî¢ Model dtype: {model.dtype}")
        print(f"   üíæ Future runs will be faster as files are cached locally")
        
    except Exception as final_error:
        print(f"‚ùå Final loading attempt failed: {final_error}")
        print("\nüîß To fix this completely:")
        print("   1. Ensure you have internet connection")
        print("   2. The model will download missing base files to your E: drive")
        print("   3. After first successful run, it will work offline")
        raise final_error

# 4Ô∏è‚É£ (Optional) Save to your own HF repo so you never patch again
# processor.save_pretrained("your-username/colqwen2-patched")

def create_collection(collection_name):
  qdrant_client.create_collection(
    collection_name=collection_name,
    vectors_config={
        "original": models.VectorParams(
            size=128,
            distance=models.Distance.COSINE,
            multivector_config=models.MultiVectorConfig(
                comparator=models.MultiVectorComparator.MAX_SIM
            ),
            on_disk=False  # High-priority vector kept in RAM
        ),
        "mean_pooling_columns": models.VectorParams(
            size=128,
            distance=models.Distance.COSINE,
            multivector_config=models.MultiVectorConfig(
                comparator=models.MultiVectorComparator.MAX_SIM
            ),
            on_disk=True  # Store on disk to save memory
        ),
        "mean_pooling_rows": models.VectorParams(
            size=128,
            distance=models.Distance.COSINE,
            multivector_config=models.MultiVectorConfig(
                comparator=models.MultiVectorComparator.MAX_SIM
            ),
            on_disk=True  # Store on disk to save memory
        )
    },
    hnsw_config=models.HnswConfigDiff(
        m=32,
        ef_construct=200
    ),
    optimizers_config=models.OptimizersConfigDiff(
        deleted_threshold=0.2,               # Optimize segments with >20% deleted points
        indexing_threshold=10000,            # Delay index building until enough data
        memmap_threshold=200000,             # Use disk-backed storage beyond ~200MB
        max_optimization_threads=4           # Limit background optimization threads
    )
)

try:
    #Payload Indices - wrapped in try-except to handle if collections don't exist
    qdrant_client.create_payload_index(
        collection_name="clothing",
        field_name="subcategory",
        field_schema=models.PayloadSchemaType.KEYWORD
    )

    qdrant_client.create_payload_index(
        collection_name="clothing",
        field_name="color",
        field_schema=models.PayloadSchemaType.KEYWORD
    )

    qdrant_client.create_payload_index(
        collection_name="jewellery",
        field_name="subcategory",
        field_schema=models.PayloadSchemaType.KEYWORD
    )

    qdrant_client.create_payload_index(
        collection_name="jewellery",
        field_name="material",
        field_schema=models.PayloadSchemaType.KEYWORD
    )

    qdrant_client.create_payload_index(
        collection_name="cosmetics",
        field_name="subcategory",
        field_schema=models.PayloadSchemaType.KEYWORD
    )

    qdrant_client.create_payload_index(
        collection_name="cosmetics",
        field_name="shade",
        field_schema=models.PayloadSchemaType.KEYWORD
    )
except Exception as e:
    print(f"‚ö†Ô∏è  Note: Some payload indices couldn't be created (collections might not exist yet): {e}")

def iterate_dataset(dataset):
    for clothing_type, subcats in dataset.get("clothings", {}).items():
        for color, links in subcats.items():
            for link in links:
                yield "clothing", clothing_type, color, link

    for subcat, shades in dataset.get("cosmetics", {}).items():
        for shade, links in shades.items():
            for link in links:
                yield "cosmetics", subcat, shade, link

    for subcat, materials in dataset.get("jewellery", {}).items():
        for material, links in materials.items():
            for link in links:
                yield "jewellery", subcat, material, link

def url_to_image(url: str):
    """Download an image from a URL and return as PIL.Image"""
    try:
        response = requests.get(url, timeout=10)
        img = Image.open(BytesIO(response.content)).convert("RGB")
        return img
    except Exception as e:
        print(f"‚ùå Failed to fetch image {url}: {e}")
        return None

def get_patches(image_size, model_processor, model):
    
    return model_processor.get_n_patches(image_size, spatial_merge_size=model.spatial_merge_size)

def embed_and_mean_pool_batch(image_batch, model_processor, model, model_name):
    """Embed a batch of images and compute mean pooling rows/columns"""
    with torch.no_grad():
        processed_images = model_processor.process_images(image_batch).to(model.device)
        image_embeddings = model(**processed_images)

    image_embeddings_batch = image_embeddings.cpu().float().numpy().tolist()
    pooled_by_rows_batch, pooled_by_columns_batch = [], []

    for image_embedding, tokenized_image, image in zip(image_embeddings,
                                                       processed_images.input_ids,
                                                       image_batch):

        image_size = image.shape[:2] if isinstance(image, np.ndarray) else image.size
        x_patches, y_patches = get_patches(image_size, model_processor, model)

        image_tokens_mask = (tokenized_image == model_processor.image_token_id)
        image_tokens = image_embedding[image_tokens_mask].view(x_patches, y_patches, model.dim)

        pooled_by_rows = torch.mean(image_tokens, dim=0)
        pooled_by_columns = torch.mean(image_tokens, dim=1)

        image_token_idxs = torch.nonzero(image_tokens_mask.int(), as_tuple=False)
        first_image_token_idx = image_token_idxs[0].cpu().item()
        last_image_token_idx = image_token_idxs[-1].cpu().item()

        prefix_tokens = image_embedding[:first_image_token_idx]
        postfix_tokens = image_embedding[last_image_token_idx + 1:]

        pooled_by_rows = torch.cat((prefix_tokens, pooled_by_rows, postfix_tokens), dim=0).cpu().float().numpy().tolist()
        pooled_by_columns = torch.cat((prefix_tokens, pooled_by_columns, postfix_tokens), dim=0).cpu().float().numpy().tolist()

        pooled_by_rows_batch.append(pooled_by_rows)
        pooled_by_columns_batch.append(pooled_by_columns)

    return image_embeddings_batch, pooled_by_rows_batch, pooled_by_columns_batch

def upload_batch(original_batch, pooled_by_rows_batch, pooled_by_columns_batch, payload_batch, collection_name):
    """Upload one batch to Qdrant"""
    for payload in payload_batch:
        if "image_link" not in payload:
            raise ValueError(f"Payload missing image_link: {payload}")

    try:
        qdrant_client.upload_collection(
            collection_name=collection_name,
            vectors={
                "original": original_batch,
                "mean_pooling_rows": pooled_by_rows_batch,
                "mean_pooling_columns": pooled_by_columns_batch
            },
            payload=payload_batch,
            ids=[str(uuid.uuid4()) for _ in range(len(original_batch))]
        )
    except Exception as e:
        print(f"‚ùå Error during upsert: {e}")

def batch_embed_query(query_batch, model_processor, model):
    with torch.no_grad():
        processed_queries = model_processor.process_queries(query_batch).to(model.device)
        query_embeddings_batch = model(**processed_queries)
    print(query_embeddings_batch.cpu().float().numpy())
    return query_embeddings_batch.cpu().float().numpy()

def reranking_search_batch(query_batch,
                           collection_name,
                           FilterList,
                           FilterColor=None,
                           search_limit=20,
                           prefetch_limit=200
                           ):
    filter_ = None

    if FilterList:
        conditions = [
            models.FieldCondition(
                key="subcategory",
                match=models.MatchAny(any=FilterList),
            )
        ]
        
        if FilterColor:
            conditions.append(
                models.FieldCondition(
                    key="color",
                    match=models.MatchAny(any=FilterColor),
                )
            )
        
        filter_ = models.Filter(should=conditions)

    search_queries = [
        models.QueryRequest(
            query=query,
            prefetch=[
                models.Prefetch(
                    query=query,
                    limit=prefetch_limit,
                    using="mean_pooling_columns"
                ),
                models.Prefetch(
                    query=query,
                    limit=prefetch_limit,
                    using="mean_pooling_rows"
                ),
            ],
            filter=filter_,
            limit=search_limit,
            with_payload=True,
            with_vector=False,
            using="original"
        ) for query in query_batch
    ]

    response = qdrant_client.query_batch_points(
        collection_name=collection_name,
        requests=search_queries
    )

    if all(not res.points for res in response):
        search_queries = [
            models.QueryRequest(
                query=query,
                prefetch=[
                    models.Prefetch(
                        query=query,
                        limit=prefetch_limit,
                        using="mean_pooling_columns"
                    ),
                    models.Prefetch(
                        query=query,
                        limit=prefetch_limit,
                        using="mean_pooling_rows"
                    ),
                ],
                filter=None,
                limit=search_limit,
                with_payload=True,
                with_vector=False,
                using="original"
            ) for query in query_batch
        ]
        response = qdrant_client.query_batch_points(
            collection_name=collection_name,
            requests=search_queries
        )

    return response

#Example usage
query = ["formal shirts with dark color"]



print("üß™ Testing model with query:", query)
colqwen_query = batch_embed_query(query, processor, model)

with torch.no_grad():
    batch_query = processor.process_queries(query).to(model.device)
    print("üî§ Tokenized query:", processor.tokenizer.tokenize(
        processor.decode(batch_query.input_ids[0])
    ))

try:
    answer_colqwen = reranking_search_batch(colqwen_query, "clothing", ["formals"])
    print("üîç Search results:", answer_colqwen[0].points)
except Exception as e:
    print(f"‚ö†Ô∏è  Search failed (this is normal if collections don't exist yet): {e}")

print("‚úÖ Script completed successfully!")
print("üéØ Next time you run this script, it will skip the download and go straight to query processing!")